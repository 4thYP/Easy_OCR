{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36ab467b-55a0-4287-9ebb-483ae36b0c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing OCR on 23138.pdf using EasyOCR...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.4.4/libexec/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[debug] OCR'd Page 1: 1103 chars, 4 text blocks\n",
      "[debug] OCR'd Page 2: 690 chars, 9 text blocks\n",
      "Reading ground truth from Shohan Nayak.docx...\n",
      "\n",
      "Saved full OCR to 23136_ocr_easyocr.txt\n",
      "\n",
      "============================================================\n",
      "OCR ACCURACY ANALYSIS REPORT (EasyOCR)\n",
      "============================================================\n",
      "Page 1: Error Rate = 96.92%, Accuracy = 3.08%\n",
      "Page 2: Error Rate = 99.03%, Accuracy = 0.97%\n",
      "\n",
      "Overall: Error Rate = 96.99%, Accuracy = 3.01%\n",
      "Matched Characters: 50 / 1709\n",
      "\n",
      "============================================================\n",
      "DETAILED DIFFERENCES (Page 1)\n",
      "============================================================\n",
      "[OCR: techno main salt l -> GT: name: shohan nay]ak[OCR: e formerly techno india, salt lake ) name_ _shalan_nayak roll no. -> GT:  roll no.:] 13[OCR: . -> MISSING]0[OCR: . -> MISSING]30[OCR: 4. -> GT: 82]3[OCR: 2. -> MISSING]1[OCR: . -> MISSING]3[OCR: .. stream_  -> GT: 8 part a classification in regression are the two most common supervised tas]k[OCR: el hm) (m imdstsriess ( pcc-alml6ot) su -> GT: s. to check the accuracy of the prediction of a validation set. there are two model parameters are there in a linear regression pro]b[OCR: ject alfliczlun : llaclis kar-semester_ 6 4 -> GT: lem with a single feature varia]b[OCR:  in -> GT: le. auc ]v[OCR: igilators signature date ,.89a2e25 pant a loxili ckm  regoasizn au hhue k most comm6n ufbzuiged dasks. preogrebes  4 asa_lialzzah set t chcek the accujack 9  tuu ao tue mde ( pastame erx a thee (7 a gmea suasuwion kolleni wuith 6 a simgle eaturs vanable_ aea aue valu  0_ jerkct - clawridre? 5 -> GT: alue of a perfect classifier is] 90[OCR: ...\n",
      "\n",
      "Saved detailed comparison report to ocr_comparison_report_easyocr.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import easyocr\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "from difflib import SequenceMatcher\n",
    "import docx\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def ocr_pdf(pdf_path, lang=\"en\", dpi=350, first_page=None, last_page=None):\n",
    "    \"\"\"\n",
    "    Perform OCR on PDF using EasyOCR\n",
    "    \"\"\"\n",
    "    # Initialize EasyOCR reader\n",
    "    reader = easyocr.Reader([lang])\n",
    "    \n",
    "    pages = convert_from_path(pdf_path, dpi=dpi, first_page=first_page, last_page=last_page)\n",
    "    page_texts = []\n",
    "    \n",
    "    for i, page in enumerate(pages, 1):\n",
    "        # Convert PIL image to numpy array\n",
    "        img_array = np.array(page)\n",
    "        \n",
    "        # Perform OCR\n",
    "        results = reader.readtext(img_array, detail=0, paragraph=True)\n",
    "        \n",
    "        # Combine all text blocks\n",
    "        page_text = \"\\n\".join(results)\n",
    "        page_texts.append(page_text)\n",
    "        \n",
    "        print(f\"[debug] OCR'd Page {i}: {len(page_text)} chars, {len(results)} text blocks\")\n",
    "    \n",
    "    return page_texts\n",
    "\n",
    "def read_docx(file_path):\n",
    "    \"\"\"Read text from a Word document\"\"\"\n",
    "    doc = docx.Document(file_path)\n",
    "    full_text = []\n",
    "    for paragraph in doc.paragraphs:\n",
    "        full_text.append(paragraph.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text by removing extra whitespace, normalizing case, \n",
    "    and removing special characters for better comparison\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove special characters but keep basic punctuation\n",
    "    text = re.sub(r'[^\\w\\s.,;:!?()\\-]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def calculate_error_metrics(ocr_text, ground_truth):\n",
    "    \"\"\"\n",
    "    Calculate various error metrics between OCR text and ground truth\n",
    "    \"\"\"\n",
    "    # Preprocess both texts\n",
    "    ocr_clean = preprocess_text(ocr_text)\n",
    "    gt_clean = preprocess_text(ground_truth)\n",
    "    \n",
    "    # Calculate similarity ratio\n",
    "    matcher = SequenceMatcher(None, ocr_clean, gt_clean)\n",
    "    similarity_ratio = matcher.ratio()\n",
    "    \n",
    "    # Calculate error rate\n",
    "    error_rate = (1 - similarity_ratio) * 100\n",
    "    \n",
    "    # Find matching blocks and differences\n",
    "    matching_blocks = matcher.get_matching_blocks()\n",
    "    total_chars = max(len(ocr_clean), len(gt_clean))\n",
    "    matched_chars = sum(block.size for block in matching_blocks)\n",
    "    \n",
    "    return {\n",
    "        'error_rate': error_rate,\n",
    "        'accuracy': similarity_ratio * 100,\n",
    "        'matched_chars': matched_chars,\n",
    "        'total_chars': total_chars,\n",
    "        'matcher': matcher,\n",
    "        'ocr_clean': ocr_clean,\n",
    "        'gt_clean': gt_clean\n",
    "    }\n",
    "\n",
    "def highlight_differences(ocr_text, ground_truth, matcher):\n",
    "    \"\"\"\n",
    "    Generate a text with highlights showing differences between OCR and ground truth\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for tag, i1, i2, j1, j2 in matcher.get_opcodes():\n",
    "        if tag == 'equal':\n",
    "            result.append(ground_truth[j1:j2])\n",
    "        elif tag == 'replace':\n",
    "            result.append(f\"[OCR: {ocr_text[i1:i2]} -> GT: {ground_truth[j1:j2]}]\")\n",
    "        elif tag == 'delete':\n",
    "            result.append(f\"[OCR: {ocr_text[i1:i2]} -> MISSING]\")\n",
    "        elif tag == 'insert':\n",
    "            result.append(f\"[MISSING IN OCR -> GT: {ground_truth[j1:j2]}]\")\n",
    "    return ''.join(result)\n",
    "\n",
    "def main():\n",
    "    # Perform OCR on the PDF using EasyOCR\n",
    "    pdf_file = \"23138.pdf\"\n",
    "    print(f\"Performing OCR on {pdf_file} using EasyOCR...\")\n",
    "    ocr_pages = ocr_pdf(pdf_file)\n",
    "    \n",
    "    # Read ground truth from Word document\n",
    "    gt_file = \"Shohan Nayak.docx\"\n",
    "    print(f\"Reading ground truth from {gt_file}...\")\n",
    "    ground_truth = read_docx(gt_file)\n",
    "    \n",
    "    # Save OCR results\n",
    "    full_ocr_text = \"\".join(f\"\\n\\n--- Page {i} ---\\n{t}\" for i, t in enumerate(ocr_pages, 1))\n",
    "    with open(\"23138_ocr_easyocr.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(full_ocr_text)\n",
    "    print(\"\\nSaved full OCR to 23136_ocr_easyocr.txt\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"OCR ACCURACY ANALYSIS REPORT (EasyOCR)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Calculate metrics for each page\n",
    "    page_metrics = []\n",
    "    for i, page_text in enumerate(ocr_pages, 1):\n",
    "        metrics = calculate_error_metrics(page_text, ground_truth)\n",
    "        page_metrics.append(metrics)\n",
    "        print(f\"Page {i}: Error Rate = {metrics['error_rate']:.2f}%, Accuracy = {metrics['accuracy']:.2f}%\")\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    combined_ocr = \" \".join(ocr_pages)\n",
    "    overall_metrics = calculate_error_metrics(combined_ocr, ground_truth)\n",
    "    \n",
    "    print(f\"\\nOverall: Error Rate = {overall_metrics['error_rate']:.2f}%, Accuracy = {overall_metrics['accuracy']:.2f}%\")\n",
    "    print(f\"Matched Characters: {overall_metrics['matched_chars']} / {overall_metrics['total_chars']}\")\n",
    "    \n",
    "    # Show detailed differences for the first page\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DETAILED DIFFERENCES (Page 1)\")\n",
    "    print(\"=\"*60)\n",
    "    diff_text = highlight_differences(\n",
    "        page_metrics[0]['ocr_clean'], \n",
    "        page_metrics[0]['gt_clean'], \n",
    "        page_metrics[0]['matcher']\n",
    "    )\n",
    "    print(diff_text[:1000] + \"...\" if len(diff_text) > 1000 else diff_text)\n",
    "    \n",
    "    # Save detailed comparison to file\n",
    "    with open(\"ocr_comparison_report_easyocr.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"OCR ACCURACY ANALYSIS REPORT (EasyOCR)\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        for i, metrics in enumerate(page_metrics, 1):\n",
    "            f.write(f\"Page {i}: Error Rate = {metrics['error_rate']:.2f}%, Accuracy = {metrics['accuracy']:.2f}%\\n\")\n",
    "        f.write(f\"\\nOverall: Error Rate = {overall_metrics['error_rate']:.2f}%, Accuracy = {overall_metrics['accuracy']:.2f}%\\n\")\n",
    "        f.write(f\"Matched Characters: {overall_metrics['matched_chars']} / {overall_metrics['total_chars']}\\n\\n\")\n",
    "        \n",
    "        f.write(\"DETAILED DIFFERENCES\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        for i, metrics in enumerate(page_metrics, 1):\n",
    "            f.write(f\"\\n--- Page {i} Differences ---\\n\")\n",
    "            diff = highlight_differences(metrics['ocr_clean'], metrics['gt_clean'], metrics['matcher'])\n",
    "            f.write(diff + \"\\n\")\n",
    "    \n",
    "    print(\"\\nSaved detailed comparison report to ocr_comparison_report_easyocr.txt\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d39a8b-f82c-4777-9cd1-b835ee84f39a",
   "metadata": {},
   "source": [
    "# Word Error Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4c97db5-99c7-4eff-895b-4f02aff7f45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing OCR on Ideal Text.pdf using EasyOCR...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.4.4/libexec/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[debug] OCR'd Page 1: 249 chars, 10 text blocks\n",
      "Reading ground truth from Ideal Gound Truth.docx...\n",
      "\n",
      "Saved full OCR to Text.txt\n",
      "\n",
      "============================================================\n",
      "OCR ACCURACY ANALYSIS REPORT (WER, EasyOCR)\n",
      "============================================================\n",
      "Page 1: WER = 104.00%  (S=49, D=0, I=3, N=50)\n",
      "\n",
      "Overall: WER = 104.00%  (S=49, D=0, I=3, N=50)\n",
      "\n",
      "============================================================\n",
      "DETAILED WORD-LEVEL DIFFERENCES (Page 1)\n",
      "============================================================\n",
      "[OCR: move -> GT: a] [OCR: j0 -> GT: move] [OCR: 3o8 -> GT: to] [OCR: ar -> GT: stop] [OCR: _ -> GT: mr] [OCR: gibbel -> GT: gaitskell] [OCR: fven -> GT: from] [OCR: wowinal -> GT: nominating] [OCR: n3 -> GT: any] [OCR: a -> GT: more] [OCR: lone_ -> GT: labour] [OCR: labaur -> GT: life] [OCR: lfe_ -> GT: peers] [OCR: per -> GT: is] [OCR: lo -> GT: to] [OCR: o_ -> GT: be] [OCR: luadr_ -> GT: made] at [EXTRA OCR: meohia] [EXTRA OCR: ak] [EXTRA OCR: labcuv] [OCR: m -> GT: a] [OCR: donorrol -> GT: meeting] [OCR: ar -> GT: of] [OCR: aclae -> GT: labour] [OCR: fool -> GT: m] [OCR: loa -> GT: ps] [OCR: pux -> GT: tomorrow] [OCR: dowsu -> GT: mr] [OCR: 0l -> GT: michael] [OCR: vejoluhot -> GT: foot] [OCR: ol -> GT: has] [OCR: lls -> GT: put] [OCR: _ -> GT: down] [OCR: suqeak -> GT: a] [OCR: o40 -> GT: resolution] [OCR: wq -> GT: on] [OCR: _ -> GT: the] [OCR: 80 -> GT: subject] [OCR: qq -> GT: and] [OCR: _ -> GT: he] [OCR: baclea -> GT: is] [OCR: 6r -> GT: to] [OCR: ar -> GT: be] [OCR: w -> GT:...\n",
      "\n",
      "Saved detailed comparison report to Report.txt\n"
     ]
    }
   ],
   "source": [
    "pdf = \"Ideal Text.pdf\"\n",
    "ground_truth = \"Ideal Gound Truth.docx\"\n",
    "Text = \"Text.txt\"\n",
    "Report = \"Report.txt\"\n",
    "\n",
    "import re\n",
    "import easyocr\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "import docx\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# ---------- OCR + IO ----------\n",
    "\n",
    "def ocr_pdf(pdf_path, lang=\"en\", dpi=350, first_page=None, last_page=None):\n",
    "    \"\"\"\n",
    "    Perform OCR on PDF using EasyOCR\n",
    "    \"\"\"\n",
    "    # Initialize EasyOCR reader\n",
    "    reader = easyocr.Reader([lang])\n",
    "\n",
    "    pages = convert_from_path(pdf_path, dpi=dpi, first_page=first_page, last_page=last_page)\n",
    "    page_texts = []\n",
    "\n",
    "    for i, page in enumerate(pages, 1):\n",
    "        # Convert PIL image to numpy array\n",
    "        img_array = np.array(page)\n",
    "\n",
    "        # Perform OCR\n",
    "        results = reader.readtext(img_array, detail=0, paragraph=True)\n",
    "\n",
    "        # Combine all text blocks\n",
    "        page_text = \"\\n\".join(results)\n",
    "        page_texts.append(page_text)\n",
    "\n",
    "        print(f\"[debug] OCR'd Page {i}: {len(page_text)} chars, {len(results)} text blocks\")\n",
    "\n",
    "    return page_texts\n",
    "\n",
    "def read_docx(file_path):\n",
    "    \"\"\"Read text from a Word document\"\"\"\n",
    "    doc = docx.Document(file_path)\n",
    "    full_text = []\n",
    "    for paragraph in doc.paragraphs:\n",
    "        full_text.append(paragraph.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "# ---------- Preprocessing ----------\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Normalize text lightly: lowercase, collapse whitespace, remove non-word punctuation\n",
    "    (keep basic punctuation, though WER uses word tokens only).\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s.,;:!?()\\-]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def tokenize_words(text):\n",
    "    \"\"\"\n",
    "    Tokenize into words for WER (alphanumeric + underscore).\n",
    "    \"\"\"\n",
    "    return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "# ---------- WER (word-level edit distance with ops) ----------\n",
    "\n",
    "def wer_with_ops(hyp_words, ref_words):\n",
    "    \"\"\"\n",
    "    Compute WER with dynamic programming and return:\n",
    "    - S (substitutions), D (deletions), I (insertions), N (# reference words)\n",
    "    - ops: list of (op, ref_word, hyp_word) where op in {\"equal\",\"sub\",\"del\",\"ins\"}\n",
    "    \"\"\"\n",
    "    m, n = len(ref_words), len(hyp_words)\n",
    "\n",
    "    # dp[i][j] = minimum edits to convert ref[0:i] -> hyp[0:j]\n",
    "    dp = [[0]*(n+1) for _ in range(m+1)]\n",
    "    back = [[None]*(n+1) for _ in range(m+1)]\n",
    "\n",
    "    # Initialize\n",
    "    for i in range(1, m+1):\n",
    "        dp[i][0] = i\n",
    "        back[i][0] = ('del', i-1, None)  # delete ref[i-1]\n",
    "    for j in range(1, n+1):\n",
    "        dp[0][j] = j\n",
    "        back[0][j] = ('ins', None, j-1)  # insert hyp[j-1]\n",
    "\n",
    "    # Fill\n",
    "    for i in range(1, m+1):\n",
    "        for j in range(1, n+1):\n",
    "            if ref_words[i-1] == hyp_words[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1]\n",
    "                back[i][j] = ('eq', i-1, j-1)\n",
    "            else:\n",
    "                sub_cost = dp[i-1][j-1] + 1\n",
    "                ins_cost = dp[i][j-1] + 1\n",
    "                del_cost = dp[i-1][j] + 1\n",
    "                best = min(sub_cost, ins_cost, del_cost)\n",
    "                dp[i][j] = best\n",
    "                if best == sub_cost:\n",
    "                    back[i][j] = ('sub', i-1, j-1)\n",
    "                elif best == ins_cost:\n",
    "                    back[i][j] = ('ins', i, j-1)\n",
    "                else:\n",
    "                    back[i][j] = ('del', i-1, j)\n",
    "\n",
    "    # Backtrack to get operations\n",
    "    ops = []\n",
    "    i, j = m, n\n",
    "    while i > 0 or j > 0:\n",
    "        op, ii, jj = back[i][j] if back[i][j] is not None else (None, None, None)\n",
    "        if op == 'eq':\n",
    "            ops.append(('equal', ref_words[ii], hyp_words[jj]))\n",
    "            i, j = ii, jj\n",
    "        elif op == 'sub':\n",
    "            ops.append(('sub', ref_words[ii], hyp_words[jj]))\n",
    "            i, j = ii, jj\n",
    "        elif op == 'ins':\n",
    "            ops.append(('ins', None, hyp_words[jj]))\n",
    "            j = jj\n",
    "        elif op == 'del':\n",
    "            ops.append(('del', ref_words[ii], None))\n",
    "            i = ii\n",
    "        else:\n",
    "            # Fallbacks\n",
    "            if i > 0:\n",
    "                ops.append(('del', ref_words[i-1], None))\n",
    "                i -= 1\n",
    "            elif j > 0:\n",
    "                ops.append(('ins', None, hyp_words[j-1]))\n",
    "                j -= 1\n",
    "\n",
    "    ops.reverse()\n",
    "\n",
    "    # Count S/D/I\n",
    "    S = sum(1 for o,_,_ in ops if o == 'sub')\n",
    "    D = sum(1 for o,_,_ in ops if o == 'del')\n",
    "    I = sum(1 for o,_,_ in ops if o == 'ins')\n",
    "    N = max(1, m)  # avoid div-by-zero; if ref empty, set N=1 by convention\n",
    "\n",
    "    return S, D, I, N, ops\n",
    "\n",
    "def calculate_wer_metrics(ocr_text, ground_truth):\n",
    "    \"\"\"\n",
    "    Calculate WER metrics between OCR text and ground truth, returning:\n",
    "    - wer_percent, S, D, I, N, ops, ref_words, hyp_words\n",
    "    \"\"\"\n",
    "    # Preprocess and tokenize\n",
    "    ocr_clean = preprocess_text(ocr_text)\n",
    "    gt_clean = preprocess_text(ground_truth)\n",
    "\n",
    "    hyp_words = tokenize_words(ocr_clean)\n",
    "    ref_words = tokenize_words(gt_clean)\n",
    "\n",
    "    S, D, I, N, ops = wer_with_ops(hyp_words, ref_words)\n",
    "    wer = (S + D + I) / N * 100.0\n",
    "\n",
    "    return {\n",
    "        'wer_percent': wer,\n",
    "        'S': S,\n",
    "        'D': D,\n",
    "        'I': I,\n",
    "        'N': N,\n",
    "        'ops': ops,\n",
    "        'ref_words': ref_words,\n",
    "        'hyp_words': hyp_words\n",
    "    }\n",
    "\n",
    "# ---------- Highlight (word-level) ----------\n",
    "\n",
    "def highlight_differences_words(ops):\n",
    "    \"\"\"\n",
    "    Build a readable diff string at the word level using ops from wer_with_ops().\n",
    "    equal -> plain word\n",
    "    sub   -> [OCR: hyp -> GT: ref]\n",
    "    del   -> [MISSING: ref]\n",
    "    ins   -> [EXTRA OCR: hyp]\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for op, ref_w, hyp_w in ops:\n",
    "        if op == 'equal':\n",
    "            out.append(ref_w)\n",
    "        elif op == 'sub':\n",
    "            out.append(f\"[OCR: {hyp_w} -> GT: {ref_w}]\")\n",
    "        elif op == 'del':\n",
    "            out.append(f\"[MISSING: {ref_w}]\")\n",
    "        elif op == 'ins':\n",
    "            out.append(f\"[EXTRA OCR: {hyp_w}]\")\n",
    "    return ' '.join(out)\n",
    "\n",
    "# ---------- Main ----------\n",
    "\n",
    "def main():\n",
    "    # Perform OCR on the PDF using EasyOCR\n",
    "    pdf_file = pdf\n",
    "    print(f\"Performing OCR on {pdf_file} using EasyOCR...\")\n",
    "    ocr_pages = ocr_pdf(pdf_file)\n",
    "\n",
    "    # Read ground truth from Word document\n",
    "    gt_file = ground_truth\n",
    "    print(f\"Reading ground truth from {gt_file}...\")\n",
    "    gt_text = read_docx(gt_file)      # ✅ rename here\n",
    "\n",
    "    # Save OCR results\n",
    "    full_ocr_text = \"\".join(f\"\\n\\n--- Page {i} ---\\n{t}\" for i, t in enumerate(ocr_pages, 1))\n",
    "    with open(Text, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(full_ocr_text)\n",
    "    print(f\"\\nSaved full OCR to {Text}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"OCR ACCURACY ANALYSIS REPORT (WER, EasyOCR)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Page-wise WER (each page vs full ground truth, to mirror your original style)\n",
    "    page_metrics = []\n",
    "    for i, page_text in enumerate(ocr_pages, 1):\n",
    "        m = calculate_wer_metrics(page_text, gt_text)    # ✅ use gt_text\n",
    "        page_metrics.append(m)\n",
    "        print(f\"Page {i}: WER = {m['wer_percent']:.2f}%  \"\n",
    "              f\"(S={m['S']}, D={m['D']}, I={m['I']}, N={m['N']})\")\n",
    "\n",
    "    # Overall WER (all pages concatenated vs full ground truth)\n",
    "    combined_ocr = \" \".join(ocr_pages)\n",
    "    overall = calculate_wer_metrics(combined_ocr, gt_text)  # ✅ use gt_text\n",
    "    print(f\"\\nOverall: WER = {overall['wer_percent']:.2f}%  \"\n",
    "          f\"(S={overall['S']}, D={overall['D']}, I={overall['I']}, N={overall['N']})\")\n",
    "\n",
    "    # Detailed differences for the first page (word-level)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DETAILED WORD-LEVEL DIFFERENCES (Page 1)\")\n",
    "    print(\"=\"*60)\n",
    "    diff_text = highlight_differences_words(page_metrics[0]['ops'])\n",
    "    print(diff_text[:1000] + \"...\" if len(diff_text) > 1000 else diff_text)\n",
    "\n",
    "    # Save detailed comparison to file\n",
    "    with open(Report, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"OCR ACCURACY ANALYSIS REPORT (WER, EasyOCR)\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        for i, m in enumerate(page_metrics, 1):\n",
    "            f.write(f\"Page {i}: WER = {m['wer_percent']:.2f}%  \"\n",
    "                    f\"(S={m['S']}, D={m['D']}, I={m['I']}, N={m['N']})\\n\")\n",
    "        f.write(f\"\\nOverall: WER = {overall['wer_percent']:.2f}%  \"\n",
    "                f\"(S={overall['S']}, D={overall['D']}, I={overall['I']}, N={overall['N']})\\n\\n\")\n",
    "\n",
    "        f.write(\"DETAILED WORD-LEVEL DIFFERENCES\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        for i, m in enumerate(page_metrics, 1):\n",
    "            f.write(f\"\\n--- Page {i} Differences ---\\n\")\n",
    "            f.write(highlight_differences_words(m['ops']) + \"\\n\")\n",
    "\n",
    "    print(f\"\\nSaved detailed comparison report to {Report}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13206971-91f1-4fa0-aff9-4c8a0a4724b4",
   "metadata": {},
   "source": [
    "# Character Error Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66b4f5d8-5f34-409d-b43a-b6ad3f78ac43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing OCR on Ideal Text.pdf using EasyOCR...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.4.4/libexec/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[debug] OCR'd Page 1: 249 chars, 10 text blocks\n",
      "Reading ground truth from Ideal Gound Truth.docx...\n",
      "\n",
      "Saved full OCR to Text.txt\n",
      "\n",
      "============================================================\n",
      "OCR ACCURACY ANALYSIS REPORT (CER, EasyOCR)\n",
      "============================================================\n",
      "[config] CER includes spaces? False\n",
      "Page 1: CER = 60.19%  (S=100, D=19, I=5, N=206)\n",
      "\n",
      "Overall: CER = 60.19%  (S=100, D=19, I=5, N=206)\n",
      "\n",
      "============================================================\n",
      "DETAILED CHARACTER-LEVEL DIFFERENCES (Page 1)\n",
      "============================================================\n",
      "[MISSING:a]move[MISSING:t][OCR:j -> GT:o][OCR:0 -> GT:s][OCR:3 -> GT:t]o[OCR:8 -> GT:p][OCR:a -> GT:m]r[OCR:_ -> GT:.]g[MISSING:a]i[MISSING:t][OCR:b -> GT:s][OCR:b -> GT:k]e[MISSING:l]lf[OCR:v -> GT:r][OCR:e -> GT:o][OCR:n -> GT:m][OCR:w -> GT:n]o[OCR:w -> GT:m]ina[OCR:l -> GT:t][OCR:; -> GT:i]n[OCR:3 -> GT:g]a[MISSING:n][MISSING:y][OCR:l -> GT:m]o[OCR:n -> GT:r]e[EXTRA OCR:_]lab[OCR:a -> GT:o]url[MISSING:i]fe[OCR:_ -> GT:p][OCR:p -> GT:e]er[OCR:) -> GT:s][OCR:l -> GT:i][OCR:o -> GT:s][OCR:( -> GT:t]o[OCR:_ -> GT:b][OCR:l -> GT:e][OCR:u -> GT:m]ad[EXTRA OCR:r][OCR:_ -> GT:e]at[MISSING:a]me[OCR:o -> GT:e][OCR:h -> GT:t]i[MISSING:n][OCR:a -> GT:g][OCR:a -> GT:o][OCR:k -> GT:f]lab[OCR:c -> GT:o]u[OCR:v -> GT:r]m[MISSING:p][OCR:? -> GT:s][OCR:d -> GT:t]o[OCR:n -> GT:m]orro[MISSING:w][OCR:l -> GT:.][OCR:a -> GT:m]r[MISSING:.][MISSING:m][OCR:a -> GT:i]c[OCR:l -> GT:h]ae[OCR:( -> GT:l]foo[OCR:l -> GT:t][OCR:l -> GT:h][OCR:o -> GT:a][OCR:a -> GT:s]pu[OCR:x -> GT:t]dow[EXTRA OCR:s][EXTRA OCR:u]...\n",
      "\n",
      "Saved detailed comparison report to Report.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import easyocr\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "import docx\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# ---------- OCR + IO ----------\n",
    "\n",
    "def ocr_pdf(pdf_path, lang=\"en\", dpi=350, first_page=None, last_page=None):\n",
    "    \"\"\"\n",
    "    Perform OCR on PDF using EasyOCR\n",
    "    \"\"\"\n",
    "    # Initialize EasyOCR reader\n",
    "    reader = easyocr.Reader([lang])\n",
    "\n",
    "    pages = convert_from_path(pdf_path, dpi=dpi, first_page=first_page, last_page=last_page)\n",
    "    page_texts = []\n",
    "\n",
    "    for i, page in enumerate(pages, 1):\n",
    "        # Convert PIL image to numpy array\n",
    "        img_array = np.array(page)\n",
    "\n",
    "        # Perform OCR\n",
    "        results = reader.readtext(img_array, detail=0, paragraph=True)\n",
    "\n",
    "        # Combine all text blocks\n",
    "        page_text = \"\\n\".join(results)\n",
    "        page_texts.append(page_text)\n",
    "\n",
    "        print(f\"[debug] OCR'd Page {i}: {len(page_text)} chars, {len(results)} text blocks\")\n",
    "\n",
    "    return page_texts\n",
    "\n",
    "def read_docx(file_path):\n",
    "    \"\"\"Read text from a Word document\"\"\"\n",
    "    doc = docx.Document(file_path)\n",
    "    full_text = []\n",
    "    for paragraph in doc.paragraphs:\n",
    "        full_text.append(paragraph.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "# ---------- Preprocessing ----------\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Normalize text lightly: lowercase, collapse whitespace, remove non-word punctuation.\n",
    "    (CER is computed over characters after this normalization.)\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s.,;:!?()\\-]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def tokenize_chars(text, include_spaces=False):\n",
    "    \"\"\"\n",
    "    Convert text into a list of characters for CER.\n",
    "    If include_spaces=False (default), spaces are removed before tokenization.\n",
    "    \"\"\"\n",
    "    if not include_spaces:\n",
    "        text = text.replace(' ', '')\n",
    "    return list(text)\n",
    "\n",
    "# ---------- CER (character-level edit distance with ops) ----------\n",
    "\n",
    "def cer_with_ops(hyp_chars, ref_chars):\n",
    "    \"\"\"\n",
    "    Compute CER with dynamic programming and return:\n",
    "    - S (substitutions), D (deletions), I (insertions), N (# reference chars)\n",
    "    - ops: list of (op, ref_char, hyp_char) where op in {\"equal\",\"sub\",\"del\",\"ins\"}\n",
    "    \"\"\"\n",
    "    m, n = len(ref_chars), len(hyp_chars)\n",
    "\n",
    "    # dp[i][j] = minimum edits to convert ref[0:i] -> hyp[0:j]\n",
    "    dp = [[0]*(n+1) for _ in range(m+1)]\n",
    "    back = [[None]*(n+1) for _ in range(m+1)]\n",
    "\n",
    "    # Initialize\n",
    "    for i in range(1, m+1):\n",
    "        dp[i][0] = i\n",
    "        back[i][0] = ('del', i-1, None)  # delete ref[i-1]\n",
    "    for j in range(1, n+1):\n",
    "        dp[0][j] = j\n",
    "        back[0][j] = ('ins', None, j-1)  # insert hyp[j-1]\n",
    "\n",
    "    # Fill\n",
    "    for i in range(1, m+1):\n",
    "        for j in range(1, n+1):\n",
    "            if ref_chars[i-1] == hyp_chars[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1]\n",
    "                back[i][j] = ('eq', i-1, j-1)\n",
    "            else:\n",
    "                sub_cost = dp[i-1][j-1] + 1\n",
    "                ins_cost = dp[i][j-1] + 1\n",
    "                del_cost = dp[i-1][j] + 1\n",
    "                best = min(sub_cost, ins_cost, del_cost)\n",
    "                dp[i][j] = best\n",
    "                if best == sub_cost:\n",
    "                    back[i][j] = ('sub', i-1, j-1)\n",
    "                elif best == ins_cost:\n",
    "                    back[i][j] = ('ins', i, j-1)\n",
    "                else:\n",
    "                    back[i][j] = ('del', i-1, j)\n",
    "\n",
    "    # Backtrack to get operations\n",
    "    ops = []\n",
    "    i, j = m, n\n",
    "    while i > 0 or j > 0:\n",
    "        op, ii, jj = back[i][j] if back[i][j] is not None else (None, None, None)\n",
    "        if op == 'eq':\n",
    "            ops.append(('equal', ref_chars[ii], hyp_chars[jj]))\n",
    "            i, j = ii, jj\n",
    "        elif op == 'sub':\n",
    "            ops.append(('sub', ref_chars[ii], hyp_chars[jj]))\n",
    "            i, j = ii, jj\n",
    "        elif op == 'ins':\n",
    "            ops.append(('ins', None, hyp_chars[jj]))\n",
    "            j = jj\n",
    "        elif op == 'del':\n",
    "            ops.append(('del', ref_chars[ii], None))\n",
    "            i = ii\n",
    "        else:\n",
    "            # Fallbacks\n",
    "            if i > 0:\n",
    "                ops.append(('del', ref_chars[i-1], None))\n",
    "                i -= 1\n",
    "            elif j > 0:\n",
    "                ops.append(('ins', None, hyp_chars[j-1]))\n",
    "                j -= 1\n",
    "\n",
    "    ops.reverse()\n",
    "\n",
    "    # Count S/D/I\n",
    "    S = sum(1 for o,_,_ in ops if o == 'sub')\n",
    "    D = sum(1 for o,_,_ in ops if o == 'del')\n",
    "    I = sum(1 for o,_,_ in ops if o == 'ins')\n",
    "    N = max(1, m)  # avoid div-by-zero if ref empty\n",
    "\n",
    "    return S, D, I, N, ops\n",
    "\n",
    "def calculate_cer_metrics(ocr_text, ground_truth, include_spaces=False):\n",
    "    \"\"\"\n",
    "    Calculate CER metrics between OCR text and ground truth, returning:\n",
    "    - cer_percent, S, D, I, N, ops, ref_chars, hyp_chars\n",
    "    \"\"\"\n",
    "    # Preprocess\n",
    "    ocr_clean = preprocess_text(ocr_text)\n",
    "    gt_clean = preprocess_text(ground_truth)\n",
    "\n",
    "    hyp_chars = tokenize_chars(ocr_clean, include_spaces=include_spaces)\n",
    "    ref_chars = tokenize_chars(gt_clean, include_spaces=include_spaces)\n",
    "\n",
    "    S, D, I, N, ops = cer_with_ops(hyp_chars, ref_chars)\n",
    "    cer = (S + D + I) / N * 100.0\n",
    "\n",
    "    return {\n",
    "        'cer_percent': cer,\n",
    "        'S': S,\n",
    "        'D': D,\n",
    "        'I': I,\n",
    "        'N': N,\n",
    "        'ops': ops,\n",
    "        'ref_chars': ref_chars,\n",
    "        'hyp_chars': hyp_chars\n",
    "    }\n",
    "\n",
    "# ---------- Highlight (character-level) ----------\n",
    "\n",
    "def highlight_differences_chars(ops):\n",
    "    \"\"\"\n",
    "    Build a readable diff string at the character level.\n",
    "    equal -> char as-is\n",
    "    sub   -> [OCR:h -> GT:r]\n",
    "    del   -> [MISSING:r]\n",
    "    ins   -> [EXTRA OCR:h]\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for op, ref_c, hyp_c in ops:\n",
    "        if op == 'equal':\n",
    "            out.append(ref_c)\n",
    "        elif op == 'sub':\n",
    "            out.append(f\"[OCR:{hyp_c} -> GT:{ref_c}]\")\n",
    "        elif op == 'del':\n",
    "            out.append(f\"[MISSING:{ref_c}]\")\n",
    "        elif op == 'ins':\n",
    "            out.append(f\"[EXTRA OCR:{hyp_c}]\")\n",
    "    return ''.join(out)\n",
    "\n",
    "# ---------- Main ----------\n",
    "\n",
    "def main():\n",
    "    # Choose whether to include spaces in CER computation\n",
    "    INCLUDE_SPACES = False  # set True if you want spaces to count as characters\n",
    "\n",
    "    # Perform OCR on the PDF using EasyOCR\n",
    "    pdf_file = pdf\n",
    "    print(f\"Performing OCR on {pdf_file} using EasyOCR...\")\n",
    "    ocr_pages = ocr_pdf(pdf_file)\n",
    "\n",
    "    # Read ground truth from Word document\n",
    "    gt_file = ground_truth\n",
    "    print(f\"Reading ground truth from {gt_file}...\")\n",
    "    gt_text = read_docx(gt_file)   # ✅ FIX: renamed to gt_text\n",
    "\n",
    "    # Save OCR results\n",
    "    full_ocr_text = \"\".join(f\"\\n\\n--- Page {i} ---\\n{t}\" for i, t in enumerate(ocr_pages, 1))\n",
    "    with open(Text, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(full_ocr_text)\n",
    "    print(\"\\nSaved full OCR to Text.txt\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"OCR ACCURACY ANALYSIS REPORT (CER, EasyOCR)\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"[config] CER includes spaces? {INCLUDE_SPACES}\")\n",
    "\n",
    "    # Page-wise CER (each page vs full ground truth, mirroring your style)\n",
    "    page_metrics = []\n",
    "    for i, page_text in enumerate(ocr_pages, 1):\n",
    "        m = calculate_cer_metrics(page_text, gt_text, include_spaces=INCLUDE_SPACES)  # ✅ use gt_text\n",
    "        page_metrics.append(m)\n",
    "        print(f\"Page {i}: CER = {m['cer_percent']:.2f}%  \"\n",
    "              f\"(S={m['S']}, D={m['D']}, I={m['I']}, N={m['N']})\")\n",
    "\n",
    "    # Overall CER (all pages concatenated vs full ground truth)\n",
    "    combined_ocr = \" \".join(ocr_pages)\n",
    "    overall = calculate_cer_metrics(combined_ocr, gt_text, include_spaces=INCLUDE_SPACES)  # ✅ use gt_text\n",
    "    print(f\"\\nOverall: CER = {overall['cer_percent']:.2f}%  \"\n",
    "          f\"(S={overall['S']}, D={overall['D']}, I={overall['I']}, N={overall['N']})\")\n",
    "\n",
    "    # Detailed differences for the first page (character-level)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DETAILED CHARACTER-LEVEL DIFFERENCES (Page 1)\")\n",
    "    print(\"=\"*60)\n",
    "    diff_text = highlight_differences_chars(page_metrics[0]['ops'])\n",
    "    print(diff_text[:1000] + \"...\" if len(diff_text) > 1000 else diff_text)\n",
    "\n",
    "    # Save detailed comparison to file\n",
    "    with open(Report, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"OCR ACCURACY ANALYSIS REPORT (CER, EasyOCR)\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        f.write(f\"[config] CER includes spaces? {INCLUDE_SPACES}\\n\")\n",
    "        for i, m in enumerate(page_metrics, 1):\n",
    "            f.write(f\"Page {i}: CER = {m['cer_percent']:.2f}%  \"\n",
    "                    f\"(S={m['S']}, D={m['D']}, I={m['I']}, N={m['N']})\\n\")\n",
    "        f.write(f\"\\nOverall: CER = {overall['cer_percent']:.2f}%  \"\n",
    "                f\"(S={overall['S']}, D={overall['D']}, I={overall['I']}, N={overall['N']})\\n\\n\")\n",
    "\n",
    "        f.write(\"DETAILED CHARACTER-LEVEL DIFFERENCES\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        for i, m in enumerate(page_metrics, 1):\n",
    "            f.write(f\"\\n--- Page {i} Differences ---\\n\")\n",
    "            f.write(highlight_differences_chars(m['ops']) + \"\\n\")\n",
    "\n",
    "    print(\"\\nSaved detailed comparison report to Report.txt\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a3a8b0-56d8-4f12-bd55-bb6d62b47b98",
   "metadata": {},
   "source": [
    "# BLEU Error Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0bc4901-2d71-4476-a8cf-5ef28440cfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing OCR on Ideal Text.pdf using EasyOCR...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.4.4/libexec/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[debug] OCR'd Page 1: 249 chars, 10 text blocks\n",
      "Reading ground truth from Ideal Gound Truth.docx...\n",
      "\n",
      "Saved full OCR to Text.txt\n",
      "\n",
      "============================================================\n",
      "OCR ACCURACY ANALYSIS REPORT (BLEU, EasyOCR)\n",
      "============================================================\n",
      "[config] BLEU max n-gram: 4, smoothing: True\n",
      "Page 1: BLEU = 2.71%  (Error=97.29%)  BP=1.000  [p1=7.5%, p2=1.9%, p3=1.9%, p4=2.0%]  (hyp=53, ref=50)\n",
      "\n",
      "Overall: BLEU = 2.71%  (Error=97.29%)  BP=1.000  [p1=7.5%, p2=1.9%, p3=1.9%, p4=2.0%]  (hyp=53, ref=50)\n",
      "\n",
      "Saved BLEU report to Report.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import math\n",
    "import easyocr\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "import docx\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# ---------- OCR + IO ----------\n",
    "\n",
    "def ocr_pdf(pdf_path, lang=\"en\", dpi=350, first_page=None, last_page=None):\n",
    "    \"\"\"\n",
    "    Perform OCR on PDF using EasyOCR\n",
    "    \"\"\"\n",
    "    # Initialize EasyOCR reader\n",
    "    reader = easyocr.Reader([lang])\n",
    "\n",
    "    pages = convert_from_path(pdf_path, dpi=dpi, first_page=first_page, last_page=last_page)\n",
    "    page_texts = []\n",
    "\n",
    "    for i, page in enumerate(pages, 1):\n",
    "        # Convert PIL image to numpy array\n",
    "        img_array = np.array(page)\n",
    "\n",
    "        # Perform OCR\n",
    "        results = reader.readtext(img_array, detail=0, paragraph=True)\n",
    "\n",
    "        # Combine all text blocks\n",
    "        page_text = \"\\n\".join(results)\n",
    "        page_texts.append(page_text)\n",
    "\n",
    "        print(f\"[debug] OCR'd Page {i}: {len(page_text)} chars, {len(results)} text blocks\")\n",
    "\n",
    "    return page_texts\n",
    "\n",
    "def read_docx(file_path):\n",
    "    \"\"\"Read text from a Word document\"\"\"\n",
    "    doc = docx.Document(file_path)\n",
    "    full_text = []\n",
    "    for paragraph in doc.paragraphs:\n",
    "        full_text.append(paragraph.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "# ---------- Preprocessing & Tokenization ----------\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Light normalization for BLEU: lowercase, collapse whitespace, strip unusual chars\n",
    "    (we compute BLEU on word tokens).\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s.,;:!?()\\-]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def tokenize_words(text):\n",
    "    \"\"\"Tokenize into words (alphanumeric + underscore).\"\"\"\n",
    "    return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "# ---------- BLEU Implementation (no external libs) ----------\n",
    "\n",
    "def make_ngrams(tokens, n):\n",
    "    return [tuple(tokens[i:i+n]) for i in range(max(0, len(tokens)-n+1))]\n",
    "\n",
    "def modified_precision(hyp, ref, n):\n",
    "    \"\"\"\n",
    "    BLEU modified n-gram precision with clipping.\n",
    "    Returns numerator, denominator.\n",
    "    \"\"\"\n",
    "    hyp_ngrams = make_ngrams(hyp, n)\n",
    "    ref_ngrams = make_ngrams(ref, n)\n",
    "\n",
    "    if not hyp_ngrams:\n",
    "        return 0, 0\n",
    "\n",
    "    hyp_counts = Counter(hyp_ngrams)\n",
    "    ref_counts = Counter(ref_ngrams)\n",
    "\n",
    "    clipped = {g: min(c, ref_counts.get(g, 0)) for g, c in hyp_counts.items()}\n",
    "    num = sum(clipped.values())\n",
    "    den = sum(hyp_counts.values())\n",
    "    return num, den\n",
    "\n",
    "def brevity_penalty(c, r):\n",
    "    \"\"\"\n",
    "    c = length of hypothesis in words\n",
    "    r = length of reference in words\n",
    "    \"\"\"\n",
    "    if c == 0:\n",
    "        return 0.0\n",
    "    if c > r:\n",
    "        return 1.0\n",
    "    return math.exp(1 - (r / c))  # BP = e^(1 - r/c)\n",
    "\n",
    "def compute_bleu(hyp_tokens, ref_tokens, max_n=4, smoothing=True):\n",
    "    \"\"\"\n",
    "    Compute BLEU score (0..1) with equal weights for 1..max_n.\n",
    "    Uses simple add-one smoothing for zero counts.\n",
    "    \"\"\"\n",
    "    weights = [1.0 / max_n] * max_n\n",
    "\n",
    "    precisions = []\n",
    "    for n in range(1, max_n + 1):\n",
    "        num, den = modified_precision(hyp_tokens, ref_tokens, n)\n",
    "        if den == 0:\n",
    "            p_n = 0.0\n",
    "        else:\n",
    "            if num == 0 and smoothing:\n",
    "                p_n = (num + 1) / (den + 1)\n",
    "            else:\n",
    "                p_n = num / den\n",
    "        precisions.append(p_n)\n",
    "\n",
    "    if all(p == 0 for p in precisions):\n",
    "        geo_mean = 0.0\n",
    "    else:\n",
    "        sum_logs = 0.0\n",
    "        for w, p in zip(weights, precisions):\n",
    "            p = max(p, 1e-16)  # avoid log(0)\n",
    "            sum_logs += w * math.log(p)\n",
    "        geo_mean = math.exp(sum_logs)\n",
    "\n",
    "    BP = brevity_penalty(len(hyp_tokens), len(ref_tokens))\n",
    "    bleu = BP * geo_mean\n",
    "    return bleu, BP, precisions\n",
    "\n",
    "def calculate_bleu_metrics(ocr_text, ground_truth, max_n=4, smoothing=True):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      - bleu_percent (0..100), bleu_error_percent (100 - BLEU%)\n",
    "      - brevity_penalty\n",
    "      - precisions list (p1..pN as 0..1)\n",
    "      - hyp_len, ref_len\n",
    "    \"\"\"\n",
    "    hyp = tokenize_words(preprocess_text(ocr_text))\n",
    "    ref = tokenize_words(preprocess_text(ground_truth))\n",
    "    bleu, bp, precisions = compute_bleu(hyp, ref, max_n=max_n, smoothing=smoothing)\n",
    "    bleu_percent = bleu * 100.0\n",
    "    return {\n",
    "        'bleu_percent': bleu_percent,\n",
    "        'bleu_error_percent': 100.0 - bleu_percent,\n",
    "        'brevity_penalty': bp,\n",
    "        'precisions': precisions,\n",
    "        'hyp_len': len(hyp),\n",
    "        'ref_len': len(ref),\n",
    "    }\n",
    "\n",
    "# ---------- Main ----------\n",
    "\n",
    "def main():\n",
    "    # Settings\n",
    "    MAX_N = 4        # BLEU-4\n",
    "    SMOOTHING = True # add-one smoothing to avoid zero scores on short/noisy text\n",
    "\n",
    "    # Perform OCR on the PDF using EasyOCR\n",
    "    pdf_file = pdf\n",
    "    print(f\"Performing OCR on {pdf_file} using EasyOCR...\")\n",
    "    ocr_pages = ocr_pdf(pdf_file)\n",
    "\n",
    "    # ✅ FIX: use different variable names for file path and text\n",
    "    gt_file = ground_truth\n",
    "    print(f\"Reading ground truth from {gt_file}...\")\n",
    "    gt_text = read_docx(gt_file)  # renamed variable\n",
    "\n",
    "    # Save OCR results\n",
    "    full_ocr_text = \"\".join(f\"\\n\\n--- Page {i} ---\\n{t}\" for i, t in enumerate(ocr_pages, 1))\n",
    "    with open(Text, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(full_ocr_text)\n",
    "    print(\"\\nSaved full OCR to Text.txt\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"OCR ACCURACY ANALYSIS REPORT (BLEU, EasyOCR)\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"[config] BLEU max n-gram: {MAX_N}, smoothing: {SMOOTHING}\")\n",
    "\n",
    "    # Page-wise BLEU\n",
    "    page_metrics = []\n",
    "    for i, page_text in enumerate(ocr_pages, 1):\n",
    "        m = calculate_bleu_metrics(page_text, gt_text, max_n=MAX_N, smoothing=SMOOTHING)  # use gt_text\n",
    "        page_metrics.append(m)\n",
    "        pcts = \", \".join(f\"p{n}={m['precisions'][n-1]*100:.1f}%\" for n in range(1, MAX_N+1))\n",
    "        print(\n",
    "            f\"Page {i}: BLEU = {m['bleu_percent']:.2f}%  \"\n",
    "            f\"(Error={m['bleu_error_percent']:.2f}%)  \"\n",
    "            f\"BP={m['brevity_penalty']:.3f}  \"\n",
    "            f\"[{pcts}]  (hyp={m['hyp_len']}, ref={m['ref_len']})\"\n",
    "        )\n",
    "\n",
    "    # Overall BLEU\n",
    "    combined_ocr = \" \".join(ocr_pages)\n",
    "    overall = calculate_bleu_metrics(combined_ocr, gt_text, max_n=MAX_N, smoothing=SMOOTHING)  # use gt_text\n",
    "    pcts_overall = \", \".join(f\"p{n}={overall['precisions'][n-1]*100:.1f}%\" for n in range(1, MAX_N+1))\n",
    "    print(\n",
    "        f\"\\nOverall: BLEU = {overall['bleu_percent']:.2f}%  \"\n",
    "        f\"(Error={overall['bleu_error_percent']:.2f}%)  \"\n",
    "        f\"BP={overall['brevity_penalty']:.3f}  \"\n",
    "        f\"[{pcts_overall}]  (hyp={overall['hyp_len']}, ref={overall['ref_len']})\"\n",
    "    )\n",
    "\n",
    "    # Save report\n",
    "    with open(Report, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"OCR ACCURACY ANALYSIS REPORT (BLEU, EasyOCR)\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        f.write(f\"[config] BLEU max n-gram: {MAX_N}, smoothing: {SMOOTHING}\\n\\n\")\n",
    "        for i, m in enumerate(page_metrics, 1):\n",
    "            pcts = \", \".join(f\"p{n}={m['precisions'][n-1]*100:.1f}%\" for n in range(1, MAX_N+1))\n",
    "            f.write(\n",
    "                f\"Page {i}: BLEU = {m['bleu_percent']:.2f}%  \"\n",
    "                f\"(Error={m['bleu_error_percent']:.2f}%)  \"\n",
    "                f\"BP={m['brevity_penalty']:.3f}  \"\n",
    "                f\"[{pcts}]  (hyp={m['hyp_len']}, ref={m['ref_len']})\\n\"\n",
    "            )\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\n",
    "            f\"Overall: BLEU = {overall['bleu_percent']:.2f}%  \"\n",
    "            f\"(Error={overall['bleu_error_percent']:.2f}%)  \"\n",
    "            f\"BP={overall['brevity_penalty']:.3f}  \"\n",
    "            f\"[{pcts_overall}]  (hyp={overall['hyp_len']}, ref={overall['ref_len']})\\n\"\n",
    "        )\n",
    "\n",
    "    print(\"\\nSaved BLEU report to Report.txt\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800bc7f2-9463-4bbf-a86f-c29671453640",
   "metadata": {},
   "source": [
    "# ROUGE Error Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfdcac8c-8c7d-4c78-86eb-cbfa558e7025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing OCR on Ideal Text.pdf using EasyOCR...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.4.4/libexec/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[debug] OCR'd Page 1: 249 chars, 10 text blocks\n",
      "Reading ground truth from Ideal Gound Truth.docx...\n",
      "\n",
      "Saved full OCR to Text.txt\n",
      "\n",
      "============================================================\n",
      "OCR ACCURACY ANALYSIS REPORT (ROUGE, EasyOCR)\n",
      "============================================================\n",
      "Page 1: R1=8.00% (Err=92.00%)  R2=0.00% (Err=100.00%)  RL_R=6.00% (Err=94.00%)  RL_P=5.66%  RL_F1=5.83%  (hyp=53, ref=50)\n",
      "\n",
      "Overall: R1=8.00% (Err=92.00%)  R2=0.00% (Err=100.00%)  RL_R=6.00% (Err=94.00%)  RL_P=5.66%  RL_F1=5.83%  (hyp=53, ref=50)\n",
      "\n",
      "Saved ROUGE report to Report.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import math\n",
    "import easyocr\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "import docx\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# ---------- OCR + IO ----------\n",
    "\n",
    "def ocr_pdf(pdf_path, lang=\"en\", dpi=350, first_page=None, last_page=None):\n",
    "    \"\"\"\n",
    "    Perform OCR on PDF using EasyOCR\n",
    "    \"\"\"\n",
    "    # Initialize EasyOCR reader\n",
    "    reader = easyocr.Reader([lang])\n",
    "\n",
    "    pages = convert_from_path(pdf_path, dpi=dpi, first_page=first_page, last_page=last_page)\n",
    "    page_texts = []\n",
    "\n",
    "    for i, page in enumerate(pages, 1):\n",
    "        # Convert PIL image to numpy array\n",
    "        img_array = np.array(page)\n",
    "\n",
    "        # Perform OCR\n",
    "        results = reader.readtext(img_array, detail=0, paragraph=True)\n",
    "\n",
    "        # Combine all text blocks\n",
    "        page_text = \"\\n\".join(results)\n",
    "        page_texts.append(page_text)\n",
    "\n",
    "        print(f\"[debug] OCR'd Page {i}: {len(page_text)} chars, {len(results)} text blocks\")\n",
    "\n",
    "    return page_texts\n",
    "\n",
    "def read_docx(file_path):\n",
    "    \"\"\"Read text from a Word document\"\"\"\n",
    "    doc = docx.Document(file_path)\n",
    "    full_text = []\n",
    "    for paragraph in doc.paragraphs:\n",
    "        full_text.append(paragraph.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "# ---------- Preprocessing & Tokenization ----------\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Light normalization for ROUGE: lowercase, collapse whitespace, strip unusual chars\n",
    "    (we compute ROUGE on word tokens).\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s.,;:!?()\\-]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def tokenize_words(text):\n",
    "    \"\"\"Tokenize into words (alphanumeric + underscore).\"\"\"\n",
    "    return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "def make_ngrams(tokens, n):\n",
    "    return [tuple(tokens[i:i+n]) for i in range(max(0, len(tokens)-n+1))]\n",
    "\n",
    "# ---------- ROUGE-N (recall) ----------\n",
    "\n",
    "def rouge_n_recall(hyp_tokens, ref_tokens, n=1):\n",
    "    \"\"\"\n",
    "    ROUGE-N recall = (# of clipped overlapping n-grams) / (total n-grams in reference).\n",
    "    Returns recall in [0,1].\n",
    "    \"\"\"\n",
    "    ref_ngrams = make_ngrams(ref_tokens, n)\n",
    "    hyp_ngrams = make_ngrams(hyp_tokens, n)\n",
    "\n",
    "    ref_counts = Counter(ref_ngrams)\n",
    "    hyp_counts = Counter(hyp_ngrams)\n",
    "\n",
    "    overlap = 0\n",
    "    for g, rc in ref_counts.items():\n",
    "        overlap += min(rc, hyp_counts.get(g, 0))\n",
    "\n",
    "    denom = sum(ref_counts.values())\n",
    "    if denom == 0:\n",
    "        return 0.0\n",
    "    return overlap / denom\n",
    "\n",
    "# ---------- ROUGE-L (LCS-based) ----------\n",
    "\n",
    "def lcs_length(x, y):\n",
    "    \"\"\"\n",
    "    Classic DP for LCS length over token sequences.\n",
    "    \"\"\"\n",
    "    m, n = len(x), len(y)\n",
    "    dp = [[0]*(n+1) for _ in range(m+1)]\n",
    "    for i in range(m):\n",
    "        xi = x[i]\n",
    "        for j in range(n):\n",
    "            if xi == y[j]:\n",
    "                dp[i+1][j+1] = dp[i][j] + 1\n",
    "            else:\n",
    "                dp[i+1][j+1] = max(dp[i][j+1], dp[i+1][j])\n",
    "    return dp[m][n]\n",
    "\n",
    "def rouge_l_scores(hyp_tokens, ref_tokens):\n",
    "    \"\"\"\n",
    "    Compute ROUGE-L recall, precision, and F1 based on LCS.\n",
    "    Returns (R, P, F1) in [0,1].\n",
    "    \"\"\"\n",
    "    lcs = lcs_length(ref_tokens, hyp_tokens)\n",
    "    ref_len = len(ref_tokens)\n",
    "    hyp_len = len(hyp_tokens)\n",
    "\n",
    "    R = 0.0 if ref_len == 0 else lcs / ref_len\n",
    "    P = 0.0 if hyp_len == 0 else lcs / hyp_len\n",
    "    if R == 0.0 and P == 0.0:\n",
    "        F1 = 0.0\n",
    "    else:\n",
    "        F1 = (2 * R * P) / (R + P)\n",
    "    return R, P, F1\n",
    "\n",
    "# ---------- ROUGE metrics wrapper ----------\n",
    "\n",
    "def calculate_rouge_metrics(ocr_text, ground_truth):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      - rouge1_recall_percent, rouge2_recall_percent\n",
    "      - rougeL_recall_percent, rougeL_precision_percent, rougeL_f1_percent\n",
    "      - error rates for recalls (100 - recall%)\n",
    "      - lengths\n",
    "    \"\"\"\n",
    "    hyp = tokenize_words(preprocess_text(ocr_text))\n",
    "    ref = tokenize_words(preprocess_text(ground_truth))\n",
    "\n",
    "    r1 = rouge_n_recall(hyp, ref, n=1)  # ROUGE-1 recall\n",
    "    r2 = rouge_n_recall(hyp, ref, n=2)  # ROUGE-2 recall\n",
    "    rl_R, rl_P, rl_F1 = rouge_l_scores(hyp, ref)\n",
    "\n",
    "    return {\n",
    "        'rouge1_recall_percent': r1 * 100.0,\n",
    "        'rouge2_recall_percent': r2 * 100.0,\n",
    "        'rougeL_recall_percent': rl_R * 100.0,\n",
    "        'rougeL_precision_percent': rl_P * 100.0,\n",
    "        'rougeL_f1_percent': rl_F1 * 100.0,\n",
    "        # \"Error rates\" as complements of recall:\n",
    "        'rouge1_error_percent': 100.0 - (r1 * 100.0),\n",
    "        'rouge2_error_percent': 100.0 - (r2 * 100.0),\n",
    "        'rougeL_error_percent': 100.0 - (rl_R * 100.0),\n",
    "        'hyp_len': len(hyp),\n",
    "        'ref_len': len(ref),\n",
    "    }\n",
    "\n",
    "# ---------- Main ----------\n",
    "\n",
    "def main():\n",
    "    # Perform OCR on the PDF using EasyOCR\n",
    "    pdf_file = pdf\n",
    "    print(f\"Performing OCR on {pdf_file} using EasyOCR...\")\n",
    "    ocr_pages = ocr_pdf(pdf_file)\n",
    "\n",
    "    # ✅ FIX: avoid variable shadowing\n",
    "    gt_file = ground_truth\n",
    "    print(f\"Reading ground truth from {gt_file}...\")\n",
    "    gt_text = read_docx(gt_file)   # <-- renamed variable\n",
    "\n",
    "    # Save OCR results\n",
    "    full_ocr_text = \"\".join(f\"\\n\\n--- Page {i} ---\\n{t}\" for i, t in enumerate(ocr_pages, 1))\n",
    "    with open(Text, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(full_ocr_text)\n",
    "    print(\"\\nSaved full OCR to Text.txt\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"OCR ACCURACY ANALYSIS REPORT (ROUGE, EasyOCR)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Page-wise ROUGE (each page vs full ground truth)\n",
    "    page_metrics = []\n",
    "    for i, page_text in enumerate(ocr_pages, 1):\n",
    "        m = calculate_rouge_metrics(page_text, gt_text)   # ✅ use gt_text\n",
    "        page_metrics.append(m)\n",
    "        print(\n",
    "            f\"Page {i}: \"\n",
    "            f\"R1={m['rouge1_recall_percent']:.2f}% (Err={m['rouge1_error_percent']:.2f}%)  \"\n",
    "            f\"R2={m['rouge2_recall_percent']:.2f}% (Err={m['rouge2_error_percent']:.2f}%)  \"\n",
    "            f\"RL_R={m['rougeL_recall_percent']:.2f}% (Err={m['rougeL_error_percent']:.2f}%)  \"\n",
    "            f\"RL_P={m['rougeL_precision_percent']:.2f}%  RL_F1={m['rougeL_f1_percent']:.2f}%  \"\n",
    "            f\"(hyp={m['hyp_len']}, ref={m['ref_len']})\"\n",
    "        )\n",
    "\n",
    "    # Overall ROUGE (all pages concatenated vs full ground truth)\n",
    "    combined_ocr = \" \".join(ocr_pages)\n",
    "    overall = calculate_rouge_metrics(combined_ocr, gt_text)  # ✅ use gt_text\n",
    "    print(\n",
    "        f\"\\nOverall: \"\n",
    "        f\"R1={overall['rouge1_recall_percent']:.2f}% (Err={overall['rouge1_error_percent']:.2f}%)  \"\n",
    "        f\"R2={overall['rouge2_recall_percent']:.2f}% (Err={overall['rouge2_error_percent']:.2f}%)  \"\n",
    "        f\"RL_R={overall['rougeL_recall_percent']:.2f}% (Err={overall['rougeL_error_percent']:.2f}%)  \"\n",
    "        f\"RL_P={overall['rougeL_precision_percent']:.2f}%  RL_F1={overall['rougeL_f1_percent']:.2f}%  \"\n",
    "        f\"(hyp={overall['hyp_len']}, ref={overall['ref_len']})\"\n",
    "    )\n",
    "\n",
    "    # Save report\n",
    "    with open(Report, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"OCR ACCURACY ANALYSIS REPORT (ROUGE, EasyOCR)\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        for i, m in enumerate(page_metrics, 1):\n",
    "            f.write(\n",
    "                f\"Page {i}: \"\n",
    "                f\"R1={m['rouge1_recall_percent']:.2f}% (Err={m['rouge1_error_percent']:.2f}%)  \"\n",
    "                f\"R2={m['rouge2_recall_percent']:.2f}% (Err={m['rouge2_error_percent']:.2f}%)  \"\n",
    "                f\"RL_R={m['rougeL_recall_percent']:.2f}% (Err={m['rougeL_error_percent']:.2f}%)  \"\n",
    "                f\"RL_P={m['rougeL_precision_percent']:.2f}%  RL_F1={m['rougeL_f1_percent']:.2f}%  \"\n",
    "                f\"(hyp={m['hyp_len']}, ref={m['ref_len']})\\n\"\n",
    "            )\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\n",
    "            f\"Overall: \"\n",
    "            f\"R1={overall['rouge1_recall_percent']:.2f}% (Err={overall['rouge1_error_percent']:.2f}%)  \"\n",
    "            f\"R2={overall['rouge2_recall_percent']:.2f}% (Err={overall['rouge2_error_percent']:.2f}%)  \"\n",
    "            f\"RL_R={overall['rougeL_recall_percent']:.2f}% (Err={overall['rougeL_error_percent']:.2f}%)  \"\n",
    "            f\"RL_P={overall['rougeL_precision_percent']:.2f}%  RL_F1={overall['rougeL_f1_percent']:.2f}%  \"\n",
    "            f\"(hyp={overall['hyp_len']}, ref={overall['ref_len']})\\n\"\n",
    "        )\n",
    "\n",
    "    print(\"\\nSaved ROUGE report to Report.txt\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dafb02-9c7c-4447-8b3e-92adc3be7aa9",
   "metadata": {},
   "source": [
    "# BERT Score Error Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbb6c19b-8d8c-4f21-8967-4f2294dc0065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing OCR on Ideal Text.pdf using EasyOCR...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.4.4/libexec/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[debug] OCR'd Page 1: 249 chars, 10 text blocks\n",
      "Reading ground truth from Ideal Gound Truth.docx...\n",
      "\n",
      "Saved full OCR to Text.txt\n",
      "\n",
      "============================================================\n",
      "OCR ACCURACY ANALYSIS REPORT (BERTScore, EasyOCR)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.4.4/libexec/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.4.4/libexec/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.4.4/libexec/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.4.4/libexec/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.4.4/libexec/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.4.4/libexec/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.4.4/libexec/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.4.4/libexec/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.4.4/libexec/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.4.4/libexec/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.4.4/libexec/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.4.4/libexec/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.4.4/libexec/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.4.4/libexec/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.4.4/libexec/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.4.4/libexec/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.4.4/libexec/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.4.4/libexec/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.4.4/libexec/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Cellar/jupyterlab/4.4.4/libexec/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1: P=-78.85% (Err=178.85%)  R=-18.80% (Err=118.80%)  F1=-50.61% (Err=150.61%)  (hyp_chars=249, ref_chars=255)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall: P=-78.85% (Err=178.85%)  R=-18.80% (Err=118.80%)  F1=-50.61% (Err=150.61%)  (hyp_chars=249, ref_chars=255)\n",
      "\n",
      "Saved BERTScore report to Report.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import math\n",
    "import easyocr\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "import docx\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# NEW: BERTScore\n",
    "from bert_score import score as bertscore_score\n",
    "\n",
    "# ---------- OCR + IO ----------\n",
    "\n",
    "def ocr_pdf(pdf_path, lang=\"en\", dpi=350, first_page=None, last_page=None):\n",
    "    \"\"\"\n",
    "    Perform OCR on PDF using EasyOCR\n",
    "    \"\"\"\n",
    "    # Initialize EasyOCR reader\n",
    "    reader = easyocr.Reader([lang])\n",
    "\n",
    "    pages = convert_from_path(pdf_path, dpi=dpi, first_page=first_page, last_page=last_page)\n",
    "    page_texts = []\n",
    "\n",
    "    for i, page in enumerate(pages, 1):\n",
    "        # Convert PIL image to numpy array\n",
    "        img_array = np.array(page)\n",
    "\n",
    "        # Perform OCR\n",
    "        results = reader.readtext(img_array, detail=0, paragraph=True)\n",
    "\n",
    "        # Combine all text blocks\n",
    "        page_text = \"\\n\".join(results)\n",
    "        page_texts.append(page_text)\n",
    "\n",
    "        print(f\"[debug] OCR'd Page {i}: {len(page_text)} chars, {len(results)} text blocks\")\n",
    "\n",
    "    return page_texts\n",
    "\n",
    "def read_docx(file_path):\n",
    "    \"\"\"Read text from a Word document\"\"\"\n",
    "    doc = docx.Document(file_path)\n",
    "    full_text = []\n",
    "    for paragraph in doc.paragraphs:\n",
    "        full_text.append(paragraph.text)\n",
    "    return '\\n'.join(full_text)\n",
    "\n",
    "# ---------- Minimal preprocessing (optional) ----------\n",
    "\n",
    "def preprocess_text_for_bertscore(text):\n",
    "    \"\"\"\n",
    "    Light normalization. BERTScore works on raw text, but normalizing whitespace\n",
    "    helps when OCR creates odd spacing.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# ---------- BERTScore metrics wrapper ----------\n",
    "\n",
    "def calculate_bertscore_metrics(ocr_text, ground_truth,\n",
    "                                lang=\"en\",\n",
    "                                model_type=None,\n",
    "                                rescale_with_baseline=True,\n",
    "                                use_idf=False):\n",
    "    \"\"\"\n",
    "    Compute BERTScore Precision, Recall, and F1 between OCR hypothesis and reference.\n",
    "    Returns % scores and % error rates (100 - score%).\n",
    "    \"\"\"\n",
    "    hyp = preprocess_text_for_bertscore(ocr_text)\n",
    "    ref = preprocess_text_for_bertscore(ground_truth)\n",
    "\n",
    "    # BERTScore expects lists of strings (cands, refs)\n",
    "    cands = [hyp]\n",
    "    refs = [ref]\n",
    "\n",
    "    P, R, F1 = bertscore_score(\n",
    "        cands,\n",
    "        refs,\n",
    "        lang=lang,\n",
    "        rescale_with_baseline=rescale_with_baseline,\n",
    "        model_type=model_type,   # None => library picks a good default for lang\n",
    "        idf=use_idf\n",
    "    )\n",
    "\n",
    "    p = float(P.mean().item())\n",
    "    r = float(R.mean().item())\n",
    "    f1 = float(F1.mean().item())\n",
    "\n",
    "    p_pct = p * 100.0\n",
    "    r_pct = r * 100.0\n",
    "    f1_pct = f1 * 100.0\n",
    "\n",
    "    return {\n",
    "        'bert_precision_percent': p_pct,\n",
    "        'bert_recall_percent': r_pct,\n",
    "        'bert_f1_percent': f1_pct,\n",
    "        'bert_precision_error_percent': 100.0 - p_pct,\n",
    "        'bert_recall_error_percent': 100.0 - r_pct,\n",
    "        'bert_f1_error_percent': 100.0 - f1_pct,\n",
    "        'hyp_len_chars': len(hyp),\n",
    "        'ref_len_chars': len(ref),\n",
    "    }\n",
    "\n",
    "# ---------- Main ----------\n",
    "\n",
    "def main():\n",
    "    # Perform OCR on the PDF using EasyOCR\n",
    "    pdf_file = pdf\n",
    "    print(f\"Performing OCR on {pdf_file} using EasyOCR...\")\n",
    "    ocr_pages = ocr_pdf(pdf_file)\n",
    "\n",
    "    # ✅ FIX: use separate variable for file and text\n",
    "    gt_file = ground_truth\n",
    "    print(f\"Reading ground truth from {gt_file}...\")\n",
    "    gt_text = read_docx(gt_file)   # renamed to avoid shadowing\n",
    "\n",
    "    # Save OCR results\n",
    "    full_ocr_text = \"\".join(f\"\\n\\n--- Page {i} ---\\n{t}\" for i, t in enumerate(ocr_pages, 1))\n",
    "    with open(Text, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(full_ocr_text)\n",
    "    print(\"\\nSaved full OCR to Text.txt\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"OCR ACCURACY ANALYSIS REPORT (BERTScore, EasyOCR)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Page-wise BERTScore (each page vs full ground truth)\n",
    "    page_metrics = []\n",
    "    for i, page_text in enumerate(ocr_pages, 1):\n",
    "        m = calculate_bertscore_metrics(\n",
    "            page_text,\n",
    "            gt_text,  # ✅ use gt_text here\n",
    "            lang=\"en\",\n",
    "            model_type=None,                # let library choose default for English\n",
    "            rescale_with_baseline=True,     # recommended\n",
    "            use_idf=False\n",
    "        )\n",
    "        page_metrics.append(m)\n",
    "        print(\n",
    "            f\"Page {i}: \"\n",
    "            f\"P={m['bert_precision_percent']:.2f}% (Err={m['bert_precision_error_percent']:.2f}%)  \"\n",
    "            f\"R={m['bert_recall_percent']:.2f}% (Err={m['bert_recall_error_percent']:.2f}%)  \"\n",
    "            f\"F1={m['bert_f1_percent']:.2f}% (Err={m['bert_f1_error_percent']:.2f}%)  \"\n",
    "            f\"(hyp_chars={m['hyp_len_chars']}, ref_chars={m['ref_len_chars']})\"\n",
    "        )\n",
    "\n",
    "    # Overall BERTScore (all pages concatenated vs full ground truth)\n",
    "    combined_ocr = \" \".join(ocr_pages)\n",
    "    overall = calculate_bertscore_metrics(\n",
    "        combined_ocr,\n",
    "        gt_text,  # ✅ use gt_text\n",
    "        lang=\"en\",\n",
    "        model_type=None,\n",
    "        rescale_with_baseline=True,\n",
    "        use_idf=False\n",
    "    )\n",
    "    print(\n",
    "        f\"\\nOverall: \"\n",
    "        f\"P={overall['bert_precision_percent']:.2f}% (Err={overall['bert_precision_error_percent']:.2f}%)  \"\n",
    "        f\"R={overall['bert_recall_percent']:.2f}% (Err={overall['bert_recall_error_percent']:.2f}%)  \"\n",
    "        f\"F1={overall['bert_f1_percent']:.2f}% (Err={overall['bert_f1_error_percent']:.2f}%)  \"\n",
    "        f\"(hyp_chars={overall['hyp_len_chars']}, ref_chars={overall['ref_len_chars']})\"\n",
    "    )\n",
    "\n",
    "    # Save report\n",
    "    with open(Report, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"OCR ACCURACY ANALYSIS REPORT (BERTScore, EasyOCR)\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        for i, m in enumerate(page_metrics, 1):\n",
    "            f.write(\n",
    "                f\"Page {i}: \"\n",
    "                f\"P={m['bert_precision_percent']:.2f}% (Err={m['bert_precision_error_percent']:.2f}%)  \"\n",
    "                f\"R={m['bert_recall_percent']:.2f}% (Err={m['bert_recall_error_percent']:.2f}%)  \"\n",
    "                f\"F1={m['bert_f1_percent']:.2f}% (Err={m['bert_f1_error_percent']:.2f}%)  \"\n",
    "                f\"(hyp_chars={m['hyp_len_chars']}, ref_chars={m['ref_len_chars']})\\n\"\n",
    "            )\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\n",
    "            f\"Overall: \"\n",
    "            f\"P={overall['bert_precision_percent']:.2f}% (Err={overall['bert_precision_error_percent']:.2f}%)  \"\n",
    "            f\"R={overall['bert_recall_percent']:.2f}% (Err={overall['bert_recall_error_percent']:.2f}%)  \"\n",
    "            f\"F1={overall['bert_f1_percent']:.2f}% (Err={overall['bert_f1_error_percent']:.2f}%)  \"\n",
    "            f\"(hyp_chars={overall['hyp_len_chars']}, ref_chars={overall['ref_len_chars']})\\n\"\n",
    "        )\n",
    "\n",
    "    print(\"\\nSaved BERTScore report to Report.txt\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
